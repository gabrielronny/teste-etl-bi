{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "from typing import Sequence, Mapping, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    "            .set(\"spark.mongodb.read.connection.uri\", \"mongodb+srv://etl-neoway:pip20po0@mongo-teste.1xrdbcs.mongodb.net/?retryWrites=true&w=majority\") \\\n",
    "            .set(\"spark.mongodb.write.connection.uri\", \"mongodb+srv://etl-neoway:pip20po0@mongo-teste.1xrdbcs.mongodb.net/?retryWrites=true&w=majority\") \\\n",
    "            .set('spark.jars.packages','org.apache.hadoop:hadoop-aws:3.2.2,com.microsoft.azure:spark-mssql-connector_2.12:1.2.0')\n",
    "\n",
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName('etl-dados-neoway') \\\n",
    "            .config(conf=conf) \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHS: Sequence[Mapping[str, Any]] = [\n",
    "    {\n",
    "        'default': 'G:\\\\Meu Drive\\\\4. Workspace\\\\teste_bi_neoway'\n",
    "    },\n",
    "    {\n",
    "        'extract': 'loads\\\\extract',\n",
    "        'transform': 'loads\\\\transform',\n",
    "        'dataCloud': 'loads\\\\dataCloud'\n",
    "    },\n",
    "    {\n",
    "        'path_bases_origem': '\\\\files\\\\spreadsheets\\\\'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G:\\\\Meu Drive\\\\4. Workspace\\\\teste_bi_neoway\\\\loads\\\\extract\\\\setores_e_ramos_de_atividades.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Movendo arquivos para extract\n",
    "\n",
    "# CNAE\n",
    "shutil.copy(f\"{PATHS[0].get('default')}\\\\{PATHS[2].get('path_bases_origem')}\\\\CNAE.csv\", f\"{PATHS[0].get('default')}\\\\{PATHS[1].get('extract')}\\\\cnae.csv\")\n",
    "\n",
    "# Empresas\n",
    "shutil.copy(f\"{PATHS[0].get('default')}\\\\{PATHS[2].get('path_bases_origem')}\\\\empresas.csv\", f\"{PATHS[0].get('default')}\\\\{PATHS[1].get('extract')}\\\\empresas.csv\")\n",
    "\n",
    "# empresasCalc \n",
    "shutil.copy(f\"{PATHS[0].get('default')}\\\\{PATHS[2].get('path_bases_origem')}\\\\empresasCalc.csv\", f\"{PATHS[0].get('default')}\\\\{PATHS[1].get('extract')}\\\\empresasCalc.csv\")\n",
    "\n",
    "# Setores e Ramos de Atividades\n",
    "shutil.copy(f\"{PATHS[0].get('default')}\\\\{PATHS[2].get('path_bases_origem')}\\\\Setores e Ramos de Atividade.csv\", f\"{PATHS[0].get('default')}\\\\{PATHS[1].get('extract')}\\\\setores_e_ramos_de_atividades.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1806373392.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 8\u001b[1;36m\u001b[0m\n\u001b[1;33m    .option(\"url\", \"jdbc:mysql://localhost/sparksql\")\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# spark.read \\\n",
    "#     .format(\"jdbc\")\n",
    "\n",
    "\n",
    "\n",
    "# sqlContext \\\n",
    "#     .read.format(\"jdbc\")\n",
    "#     .option(\"url\", \"jdbc:mysql://localhost/sparksql\") \n",
    "#     .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    "#     .option(\"dbtable\", \"baby_names\")\n",
    "#     .option(\"user\", \"root\")\n",
    "#     .option(\"password\", \"root\")\n",
    "#     .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_cnae \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mread \\\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menconding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUTF-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPATHS[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mPATHS[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextract\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mCNAE.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m df_cnae\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp_cnaes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "df_cnae = spark.read \\\n",
    "    .option(\"enconding\", \"UTF-8\") \\\n",
    "    .option(\"delimiter\", \";\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(f\"{PATHS[0].get('default')}\\\\{PATHS[1].get('extract')}\\\\CNAE.csv\")\n",
    "\n",
    "df_cnae.createOrReplaceTempView(\"temp_cnaes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        cast(trim(cd_cnpj) as string)   as cnpj,\n",
    "        trim(nu_ordem)                  as num_ordem,\n",
    "        trim(cd_ramo_atividade)         as cod_ramo_atividade,\n",
    "        trim(de_ramo_atividade)         as de_ramo_atividade\n",
    "    FROM temp_cnaes\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4113215bbbbfe342043712d8a4b0b9058f25c704a34ce5418364aca20569f2db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
